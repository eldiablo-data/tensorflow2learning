{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion MNIST with CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdIFX8pVuJdulfloZ/X8rm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eldiablo-data/tensorflow2learning/blob/master/notebooks/Fashion_MNIST_with_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "taSV3Qb_I_EA",
        "outputId": "b6df95d3-125c-4fc8-ab07-90e29e1e7048"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mDOeQnAJjOP"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6dW-AerJ3bJ"
      },
      "source": [
        "# Fashion MNIST\n",
        "This dataset is more complex than the regular MNIST dataset. It contains 10 classes of clothing and apparel object images in gray scale of size 28 x 28. We load this dataset from Keras similar to MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_mthl2OJ09B",
        "outputId": "ea7e7231-23d4-4696-f2f3-871d4528b34a"
      },
      "source": [
        "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train_scaled = x_train / 255\n",
        "x_test_scaled = x_test /255\n",
        "print(\"x_train.shape:\",x_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape: (60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq2QvNrbLkPS"
      },
      "source": [
        "The input is a 28 x 28 grayscale image. For convolution, colored images are expected (height, width and colors as 3 dimensions) and  it requires an additional dimension in the input. Hence, the scalar color is represented as a 1D vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6sVg1XvMGjK",
        "outputId": "7bcd53dc-d198-49c8-9b6e-b6988b37c142"
      },
      "source": [
        "x_train_scaled_cnn = np.expand_dims(x_train_scaled, -1)\n",
        "x_test_scaled_cnn = np.expand_dims(x_test_scaled,-1)\n",
        "print(\"x_train_scaled_cnn.shape:\",x_train_scaled_cnn.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape: (60000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO1x842MNQon",
        "outputId": "af629557-d3ba-40e6-8f4d-1d532a8b5f13"
      },
      "source": [
        "k = len(set(y_train))\n",
        "print(\"Number of classes: \", k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of classes:  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e6tTSwaOqbk"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6tR18cPOp-t"
      },
      "source": [
        "i = Input(shape=x_train_scaled_cnn[0].shape)\n",
        "x = Conv2D(filters=32,kernel_size=(3,3),strides=2, activation='relu')(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0s9V-NcPvzG"
      },
      "source": [
        "We add a 2D convolution layer of size 32 with kernel size 3x3. This is a fine layer looking into patterns and strokes. In the next layer, we will increase the filters to look for larger patterns like, say buttons and stitches. In the final layer, we will increase the filter to even larger patterns such as collars and sleeves. \n",
        "Strides make the model skip a certain number of neighbouring pixels to save computation times. Usually immediate neighbours tend to have similary color values. "
      ]
    }
  ]
}